---
title: "Large Language Models @Amsterdam"
permalink: /largelanguagemodels/
layout: single
author_profile: true
---

There is much research on Large Language Models happening in Amsterdam. Here is a selection of recent research and media reports that might be of interest.
*Er gebeurt veel onderzoek over gigantische taalmodellen (Large Language Models, Foundation Models)  in Amsterdam. Hier is selectie van onderzoekers, en journliastieke en wetenschappelijke publicaties.*

## Raquel Fernández, Jelle Zuidema, Jelke Bloem, Sandro Pezelle, Ece Takmaz, Mario Guilianelli (2023)
Public lectures on ChatGPT

<img src="../assets/images/anafternoonwithchatgpt1.jpg" alt="anafternoonwithchatgpt1" width="200"/>

![anafternoonwithchatgpt1](../assets/images/anafternoonwithchatgpt1.jpg)

## Arabella Sinclair, Jaap Jumelet, Willem Zuidema, Raquel Fernández (2022)
Research on how much knowledge of grammar is acquired by large language models.
*Onderzoek naar hoeveel kennis van grammatica-regels taalmodellen hebben geleerd.*

### Blog post 
[ILLC Blog](https://resources.illc.uva.nl/illc-blog/probing-by-priming-what-do-large-language-models-know-about-grammar/)

### Scientific publication: 
Arabella Sinclair, Jaap Jumelet, Willem Zuidema, Raquel Fernández; Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations. Transactions of the Association for Computational Linguistics 2022; 10 1031–1050. [doi](https://doi.org/10.1162/tacl_a_00504) [arxiv](https://arxiv.org/abs/2109.14989)


## Oskar van der Wal, Dominik Bachmann, Alina Leidinger, Leendert van Maanen, Willem Zuidema, Katrin Schulz (2022)
Research on undesirable biases in Large Language Models and how they can be measured and mitigated.
*Onderzoek naar ongewenste biases in taalmodellen en hoe we die kunnen meten en voorkomen.*

### Scientific publication:  
Oskar van der Wal, Dominik Bachmann, Alina Leidinger, Leendert van Maanen, Willem Zuidema, Katrin Schulz (2022), Undesirable biases in NLP: Averting a crisis of measurement. [arxiv](https://arxiv.org/abs/2211.13709)


## Oskar van der Wal, Jaap Jumelet, Katrin Schulz, Willem Zuidema 
Research on the evolution of undesirable biases in Large Language Models during training.
*Onderzoek naar het ontstaan van ongewenste biases in grote taalmodellen.*

### Scientific publication:  
Oskar van der Wal, Jaap Jumelet, Katrin Schulz, Willem Zuidema (2022), The Birth of Bias: A case study on the evolution of gender bias in an English language model. [arxiv](https://arxiv.org/abs/2207.10245)


## Oskar van der Wal & 390 other authors (2022)
The multilingual, open-source language model Bloom, created and trained by the Big Science consortium.
*Het veeltalige, open-source taalmodel BLOOM van het Big Science consortium.*

### Scientific publication: 
BigScience Workshop et al., BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, 2022.
[arxiv](https://arxiv.org/abs/2211.05100)


## Rochelle Choeni, Ekaterina Shutova & Robert van Rooij (2021) 
Onderzoek naar (ongewenste) stereotypen die opduiken in taalmodellen en zoekmachines.

### Persbericht *Press release*
[UvA persberichten](https://www.uva.nl/content/nieuws/persberichten/2021/11/welke-stereotypen-zitten-ingebakken-in-ai-taalmodellen.html?origin=8geT2goFTZSNjbHsS7pkaQ)

### Scientific publication: 
Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you? Gepresenteerd op de 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP2021), 7 november 2021. [Weblink naar de wetenschappelijke publicatie](https://aclanthology.org/2021.emnlp-main.111/).


